# **ğŸ“– ETL Service User Guide**
## **How to Configure and Run ETL Jobs Without Writing Code**

The **ETL (Extract, Transform, Load) Service** allows you to **move data between databases**, **apply transformations**, and **automate jobs**â€”all without coding. This guide explains how to **configure, schedule, and manage ETL jobs** using simple configuration files.

---

# **1ï¸âƒ£ What is the ETL Service?**
The **ETL Service** extracts data from a source (e.g., MySQL, PostgreSQL), transforms it (applying business rules), and loads it into a target database. It can also:
âœ… Synchronize schemas between databases.  
âœ… Apply **business rules** to transform data.  
âœ… Process large datasets in **batches** to prevent slowdowns.  
âœ… Run jobs **on a schedule** (e.g., every 5 minutes, hourly, daily).  
âœ… Handle **errors and retries** automatically.

---

# **2ï¸âƒ£ How to Configure an ETL Job**
Each ETL job is defined in a configuration file:  
ğŸ“‚ `config/etlConfig.json`

### **âœ… Example ETL Job Configuration**
```json
[
  {
    "source_table": "users",
    "target_table": "customers",
    "keys": ["id"],
    "frequency": "5m",
    "transformations": [
      { "field": "full_name", "expression": "data.first_name + ' ' + data.last_name" },
      { "field": "created_at", "expression": "new Date().toISOString()" }
    ],
    "notifications": [
      { "condition": "data.account_status == 'suspended'", "message": "Suspended account detected: ${data.email}" }
    ]
  }
]
```

---

# **3ï¸âƒ£ How to Set Up & Run an ETL Job**
## **ğŸ“Œ Step 1: Configure Your ETL Job**
- **Edit** the `etlConfig.json` file to define your job.
- **Specify** the source & target tables, scheduling frequency, and transformation rules.

## **ğŸ“Œ Step 2: Start the ETL Service**
No need to write any code! Simply **start the service**:

```sh
node etl_service.js
```
âœ… The service will **automatically schedule jobs** based on the configuration.

## **ğŸ“Œ Step 3: Check Job Execution Logs**
To monitor the ETL job, **check the logs**:

```sh
tail -f logs/etl.log
```
You'll see messages like:

```
Starting ETL job for users -> customers
Processing 1000 records in batch
ETL job completed successfully.
```

---

# **4ï¸âƒ£ Understanding the ETL Job Configuration**
## **ğŸ› ï¸ Key Components of `etlConfig.json`**
| Field              | Description |
|--------------------|-------------|
| `source_table`     | The table to extract data from. |
| `target_table`     | The table to load transformed data into. |
| `keys`             | The primary key(s) used to track updates. |
| `frequency`        | How often the job runs (`5m` = every 5 minutes, `1h` = hourly). |
| `transformations`  | Rules to modify data before loading it. |
| `notifications`    | Alerts for specific conditions (e.g., `account_status == 'suspended'`). |

---

# **5ï¸âƒ£ Applying Business Rules to ETL Jobs**
The ETL service allows **custom transformations** using **Business Rules**.

## **âœ… Example: Business Rules for Lead Conversion**
Create a **rules file** in ğŸ“‚ `config/rules/users_rules.dsl`:

```text
IF UPDATE users WHEN data.account_status = "suspended" THEN
    notify "Suspended account detected: ${data.email}"
ELSE IF data.account_status = "active" THEN
    update last_active_date = new Date().toISOString()
```

### **How It Works**
âœ… If `account_status = "suspended"`, send a **notification**.  
âœ… If `account_status = "active"`, update `last_active_date`.  

ğŸ’¡ **Rules are applied automatically!** No manual intervention is needed.

---

# **6ï¸âƒ£ Scheduling ETL Jobs**
### **â³ How to Set Job Frequency**
Set the `frequency` field in `etlConfig.json`:

| Frequency | Runs Every |
|-----------|-----------|
| `"30s"`   | 30 seconds |
| `"5m"`    | 5 minutes |
| `"1h"`    | 1 hour |
| `"24h"`   | 1 day |

**Example:**  
To run an ETL job **every hour**, set:
```json
"frequency": "1h"
```

---

# **7ï¸âƒ£ Error Handling & Retries**
The ETL service automatically **detects errors** and **retries failed jobs**.

### **âœ… How the ETL Service Handles Failures**
1ï¸âƒ£ If a record **fails validation**, it is **skipped** and logged.  
2ï¸âƒ£ If the **database connection fails**, the job **waits and retries**.  
3ï¸âƒ£ If an error occurs **3 times in a row**, the job is **marked as failed**.

ğŸ“Œ **Example Log Message for a Failed Job**
```
ETL job attempt 1 failed for users -> customers
Retrying in 2 seconds...
Retry successful after 1 attempts.
```

---

# **8ï¸âƒ£ How to Monitor ETL Performance**
You can track **how many records were processed**, **error rates**, and **job execution time**.

## **âœ… View ETL Metrics**
Each job logs **performance metrics**, such as:
```
ETL Job Metrics:
- Records Processed: 2,000
- Errors: 5
- Validation Errors: 2
- Execution Time: 12s
```

---

# **9ï¸âƒ£ Manually Running an ETL Job**
If you need to **run a job immediately** without waiting for the schedule:

```sh
node etl_module.js
```
This will **process all configured ETL jobs** immediately.

---

# **ğŸ”Ÿ Troubleshooting Common Issues**
### **ğŸš¨ Issue: No Data is Being Processed**
âœ… Ensure `etlConfig.json` is correctly formatted.  
âœ… Check database **connections** (`source_table`, `target_table`).  
âœ… Verify **keys** exist in the tables.  

### **ğŸš¨ Issue: Errors in Transformation Rules**
âœ… Check `config/rules/{table}_rules.dsl` for typos.  
âœ… Ensure expressions (e.g., `data.first_name + ' ' + data.last_name'`) are **valid JavaScript**.  

---

# **ğŸ¯ Summary: How to Use the ETL Service**
1ï¸âƒ£ **Edit `etlConfig.json`** â†’ Define ETL jobs.  
2ï¸âƒ£ **Start the ETL service** â†’ `node etl_service.js`.  
3ï¸âƒ£ **Monitor logs** â†’ `tail -f logs/etl.log`.  
4ï¸âƒ£ **Apply business rules** â†’ Edit `rules/{table}_rules.dsl`.  
5ï¸âƒ£ **Schedule jobs automatically** â†’ No manual execution needed!  

ğŸš€ **Now you can move, transform, and sync data across databases effortlessly!** ğŸ¯